{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Autoencoder with K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# to import CIFAR-10 as torch tensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# load the training and test datasets\n",
    "train_data = datasets.CIFAR10(root='data', train=True,\n",
    "                                   download=True, transform=transform)\n",
    "test_data = datasets.CIFAR10(root='data', train=False,\n",
    "                                  download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training and testing dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 50\n",
    "lr = 1e-3\n",
    "# how many epochs for training\n",
    "num_epochs = 40\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## autoencoder neural network design\n",
    "* input layer: 1 channel, 28 pixels wide, 28 pixels long\n",
    "* conv 1 layer: 16 channel, 10 pixels wide, 10 pixels long\n",
    "* pool 1 layer: 16 channel, 5 pixels wide, 5 pixels long\n",
    "* conv 2 layer: 32 channel, 3 pixels wide, 3 pixels long\n",
    "* pool 2 layer: 32 channel, 2 pixels wide, 2 pixels long\n",
    "\n",
    "* latent layer: 128 input, 10 output\n",
    "* upscale layer: 10 input, 128 output\n",
    "\n",
    "* deconv 3 layer: 16 channel, 5 pixels wide, 5 pixels long\n",
    "* deconv 2 layer: 8 channel, 15 pixels wide, 15 pixels long\n",
    "* deconv 1 layer: 1 channel, 28 pixels wide, 28 pixels long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cae(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(cae, self).__init__()\n",
    "        # convolutional encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 12, 4, stride=2, padding=1),            # [batch, 12, 16, 16]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(12, 24, 4, stride=2, padding=1),           # [batch, 24, 8, 8]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(24, 48, 4, stride=2, padding=1),           # [batch, 48, 4, 4]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(48, 96, 4, stride=2, padding=1),           # [batch, 96, 2, 2]\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        #autoencoder bottle neck\n",
    "        #self.latent = nn.Sequential(\n",
    "        #    nn.Linear(48*4*4, 200),\n",
    "        #    nn.ReLU(True),\n",
    "        #)\n",
    "        #self.upscale = nn.Sequential(\n",
    "        #    nn.Linear(200, 48*4*4),\n",
    "        #    nn.ReLU(True),\n",
    "        #)\n",
    "        # convolutional decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(96, 48, 4, stride=2, padding=1),  # [batch, 48, 4, 4]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(48, 24, 4, stride=2, padding=1),  # [batch, 24, 8, 8]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(24, 12, 4, stride=2, padding=1),  # [batch, 12, 16, 16]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(12, 3, 4, stride=2, padding=1),   # [batch, 3, 32, 32]\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    # forward learning path\n",
    "    def forward(self, x):\n",
    "        bottle_neck = self.encoder(x)\n",
    "        #bottle_neck = self.latent(x.view(-1, 48*4*4))\n",
    "        \n",
    "        #x = self.upscale(bottle_neck)\n",
    "        x = self.decoder(bottle_neck)\n",
    "        return x, bottle_neck.view(-1, 96*2*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAE model instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cae(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(3, 12, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(12, 24, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(24, 48, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): Conv2d(48, 96, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(96, 48, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): ConvTranspose2d(48, 24, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): ConvTranspose2d(24, 12, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): ConvTranspose2d(12, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# build a CAE model\n",
    "model = cae()\n",
    "# set loss function\n",
    "criterion = nn.MSELoss()\n",
    "# choose an optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,\n",
    "                             weight_decay=1e-7)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/40], loss:0.0160\n",
      "epoch [2/40], loss:0.0107\n",
      "epoch [3/40], loss:0.0094\n",
      "epoch [4/40], loss:0.0084\n",
      "epoch [5/40], loss:0.0077\n",
      "epoch [6/40], loss:0.0072\n",
      "epoch [7/40], loss:0.0068\n",
      "epoch [8/40], loss:0.0062\n",
      "epoch [9/40], loss:0.0059\n",
      "epoch [10/40], loss:0.0054\n",
      "epoch [11/40], loss:0.0052\n",
      "epoch [12/40], loss:0.0049\n",
      "epoch [13/40], loss:0.0050\n",
      "epoch [14/40], loss:0.0047\n",
      "epoch [15/40], loss:0.0045\n",
      "epoch [16/40], loss:0.0042\n",
      "epoch [17/40], loss:0.0041\n",
      "epoch [18/40], loss:0.0039\n",
      "epoch [19/40], loss:0.0039\n",
      "epoch [20/40], loss:0.0038\n",
      "epoch [21/40], loss:0.0036\n",
      "epoch [22/40], loss:0.0036\n",
      "epoch [23/40], loss:0.0034\n",
      "epoch [24/40], loss:0.0034\n",
      "epoch [25/40], loss:0.0033\n",
      "epoch [26/40], loss:0.0032\n",
      "epoch [27/40], loss:0.0033\n",
      "epoch [28/40], loss:0.0035\n",
      "epoch [29/40], loss:0.0030\n",
      "epoch [30/40], loss:0.0030\n",
      "epoch [31/40], loss:0.0030\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "for epoch in range(num_epochs):\n",
    "    for data in train_loader:\n",
    "        img, _ = data\n",
    "        # forward path\n",
    "        output, _ = model(img)\n",
    "        loss = criterion(output, img)\n",
    "        # back propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # log\n",
    "    print('epoch [{}/{}], loss:{:.4f}'\n",
    "          .format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualizing reconstructed results\n",
    "### NOTE TO SELF: to be automated through DV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataiter = iter(test_loader)\n",
    "images, _ = dataiter.next()\n",
    "\n",
    "# get sample outputs\n",
    "output, _ = model(images)\n",
    "# prep images for display\n",
    "images = images.numpy()# output is resized into a batch of iages\n",
    "output = output.view(batch_size, 3, 32, 32)\n",
    "# use detach when it's an output that requires_grad\n",
    "output = output.detach().numpy()\n",
    "\n",
    "# plot the first ten input images and then reconstructed images\n",
    "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25,4))\n",
    "\n",
    "# input images on top row, reconstructions on bottom\n",
    "for images, row in zip([images, output], axes):\n",
    "    for img, ax in zip(images, row):\n",
    "        ax.imshow(img.transpose((1, 2, 0)))\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reconstruct the entire dataset\n",
    "### NOTE TO SELF: eval to be used to stop regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the dataset through the trained model\n",
    "for image_index, data in enumerate(train_loader):\n",
    "    images, itr_labels = data\n",
    "    # forward pass: compute embedded outputs by passing inputs to the model\n",
    "    decoded, encoded = model(images)\n",
    "    if not image_index:\n",
    "        #First group encoded in new array\n",
    "        embedded = encoded.detach().numpy()\n",
    "        #first group decoded\n",
    "        disembedded = decoded.detach().numpy()\n",
    "        #labels\n",
    "        labels = itr_labels.detach().numpy()\n",
    "        continue\n",
    "    #stacking the remaining data\n",
    "    embedded = np.vstack((embedded, encoded.detach().numpy()))\n",
    "    disembedded = np.vstack((disembedded, decoded.detach().numpy()))\n",
    "    labels = np.hstack((labels, itr_labels.detach().numpy()))\n",
    "    print(embedded.shape)\n",
    "    print(disembedded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clustering model instantiation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# create a k-means model to cluster the embedded features\n",
    "clustering_model = KMeans(n_clusters=10, tol = 1e-4, max_iter = 400).fit(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualizing results and performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import DataVisuals as dv\n",
    "\n",
    "\n",
    "view_results = dv.DataVisuals(disembedded.reshape(-1, 3, 32, 32), labels, clustering_model.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_results.cm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_results.scat(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metric = dv.Metrics(labels, clustering_model.labels_)\n",
    "nmi = metric.nmi()\n",
    "ari = metric.ari()\n",
    "acc = metric.acc()\n",
    "print('NMI = {:.4f} \\nARI = {:.4f} \\nACC = {:.4f}'.format(nmi, ari, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## storing a copy of learned weights in hard drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
